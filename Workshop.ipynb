{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc739ed-43ce-4053-8bec-08754ada91a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# QUT EcoAcoustics Recogniser Workshop 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022292ec-f105-4872-aded-7bf93217dae2",
   "metadata": {},
   "source": [
    "## Overview of the Recogniser Building Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532b255-57a1-4430-a273-b2e43a332c59",
   "metadata": {},
   "source": [
    "### Dataset preparation \n",
    "\n",
    "Annotating audio\n",
    "\n",
    "Balancing dataset \n",
    "\n",
    "Spectrogram segment generation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902666df-cd1e-4bba-985e-6496421c9389",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training/validation split \n",
    "\n",
    "Running training \n",
    "\n",
    "error analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb89dba-2b6e-4289-b00b-c63b598cbf93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference\n",
    "Running inference \n",
    "\n",
    "Collating results \n",
    "\n",
    "inspecting verifying results \n",
    "\n",
    "labeling data for training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a57074",
   "metadata": {},
   "source": [
    "### Setting up your environment\n",
    "\n",
    "We are going to be using python for everything, including inspecting our dataset, training the recogniser, and visualising spectrograms and results. \n",
    "\n",
    "If are already reading this in your browser, you have python and jupyter installed. In python (like many other languages) there are some core functions available in the language and some that are part of packages that need to be imported before they can be used. Some of those packages are already available in the python distribution, and some packages must be installed before they can be imported. \n",
    "\n",
    "In the cell below, we install all the packages we will need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08933c68-a731-48db-8134-5509906348fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install librosa\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cb321",
   "metadata": {},
   "source": [
    "Now that we have installed the required packages, we will import them so that we can use them in our scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f285e-2e25-4d10-a1c6-5d592ca11a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "\n",
    "import importlib\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from PIL import Image\n",
    "\n",
    "import configparser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import utils\n",
    "import Spectrogram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8d6f8-8fc5-4bbc-92ff-f1957c161456",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad6465-1d8c-4cc3-a64b-c3cc510ff384",
   "metadata": {},
   "source": [
    "The recogniser is is a binary classifier, meaning it assigns one of two labels (positive or negative) to segments of audio.  In a basic Convolutional Neural Network (CNN), these segments are fixed-sized spectrogram images. In the training phase, these fixed-size segments are fed into the CNN along with their labels.  Therefore we need to be have a collection of fixed-length labelled segments. \n",
    "\n",
    "Because animal vocalisations are not of a fixed-duration, we need to deal with that somehow. There are several approaches to this: We can either resize the spectrogram image to the predetermined fixed size, distorting the image, or we can crop out as section of audio, which may discard some of the vocalisation or may add some padding audio around the vocalisation. For the recogniser we are developing today we have chosen the second method of cropping. \n",
    "\n",
    "The labelled data you have provided for this workshop is in one of two formats\n",
    "- Pre-segented\n",
    "- Annotated\n",
    "\n",
    "The annoted files have sections of audio delimited with start and end offsets, indicating where to crop from. For the \"pre-segmented\" files, we assume we can crop from anywhere. \n",
    "\n",
    "The way that this is implemented in the recogniser scripts you are using in this workshop is that \n",
    "1. A preprocessing script reads all the annotation files for your audio segments and creates a single list of all annotations\n",
    "    - for those presegmented files that don't have an associated annotation file, we know that the entire file can be treated as an annotation\n",
    "2. One at a time each annotation is selected for training\n",
    "3. A random-fixed length segment is cropped out from the audio matching the current annotation and then that is fed into the CNN\n",
    "\n",
    "\n",
    "Short annotations (less than the length of the fixed-length segment we are cropping) are cropped so that the entire annotation fits in the crop. This is illustrated below. Extra audio is included outside the annotation to acheive the fixed-duration. The blue area is the annotation and green area shows the possible time interval we can crop from. \n",
    "\n",
    "\n",
    "<img alt='short annotation' src='notebook_images/short_annotation.png' width=600 />\n",
    "\n",
    "Longer annotations (more than the length of the fixed-length segment) are cropped so that the cropped segment fits within the annotation. This is shown in the figure below. \n",
    "\n",
    "<img alt='long annotation' src='notebook_images/long_annotation.png' width=600 />\n",
    "\n",
    "\n",
    "#### Creating annotations\n",
    "\n",
    "The process described above requires a text file that contains the annotation information, i.e. information about the start and end offset of each vocalisation. There exist a few software tools that can be used to generate this file. The one that we have recommended for this workshop is Raven (or Raven Lite), but the A2O or Ecosounds, or Audacity can also perform a similar task. The resulting file is tabular data, where each row is an annotation, and two of the columns respectively contain information describing the start time (relative to the start of the file) and the end time or duration. \n",
    "\n",
    "These two tutorial videos show how to annotate audio files in Raven or Raven lite\n",
    "https://ravensoundsoftware.com/video-tutorials/english/02-selections-and-measurements/\n",
    "https://ravensoundsoftware.com/video-tutorials/english/03-saving-selection-tables/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa4b02-7080-41af-91cb-ccab23cf94b0",
   "metadata": {},
   "source": [
    "### Spectrogram Generation\n",
    " \n",
    "The CNN used is an image classifier. The sound is converted to an image in the form of a spectrogram.  The spectrogram is created by taking short (possibly overlapping) slices of the audio signal and calculating the discrete spectrum on them with a discrete Fourier transform. Each spectrum-slice then forms a column of the spectrogram, and when arranged side by side form a two dimensional grid. \n",
    "\n",
    "Each row of the grid represents a frequency band of equal frequency range. However, in nature, usually more information is encoded within a given frequency range at low frequencies than the same frequency range at higher frequencies. Therefore, most ecoacoustic CNNs use a mel-scale spectrogram, in which have a higher frequency resolution at low frequencies and a lower frequency resolution at higher frequencies. \n",
    "\n",
    "There are several parameters you can modify when generating these mel frequency spectrograms. You can edit these in the configuration file `config.ini`\n",
    "\n",
    "\n",
    "The most important of these is the `time-win` (time window). This sets how many seconds of audio are fed into the CNN. However, note that whatever value is chosen for this, the resulting spectrogram will be reshaped into a size of 128x256 pixels. Setting a very large value for the time window will result in loss of time resolution when this reshaping is done. \n",
    "\n",
    "The most important is the 'windows size', which is the number of audio samples in each slice of the audio. This determines both the time resolution (a shorter window gives a more precise point in time for the resulting column) and the frequency resolution (a longer window gives more information to determine the spectrum - the contribution of wavelengths that don't fit in the window will be ignored)\n",
    "\n",
    "You can also trim the spectrogram to include only a given range of frequencies. You may for example remove low or high frequencies well outside the range of your target species. This can both make the task easier for the model, since confusing are not included, as well as reducing the computational load since the input spectrogram size can be reduced. However it is important to remember that frequencies outside the range of the target call-type do help with discrimination between a positive and negative example\n",
    "\n",
    "\n",
    "It is important that whatever spectrogram parameters are used during training are also used when the network is deployed. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f8936-55ac-4343-afeb-9ca685e8e80d",
   "metadata": {},
   "source": [
    "### Dataset Preparation - Practical Steps\n",
    "\n",
    "We need to:\n",
    "- 1.1 Initialise configuration parameters.\n",
    "- 1.2 Prepare the training data based on how the training data is layed out on the file system.\n",
    "- 1.3 Check that the information about the training data is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ae1d6-3507-47dd-b1a2-d610df303ebd",
   "metadata": {},
   "source": [
    "#### Processing step 1.1 - Initialise the configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d47a5-fa82-4a18-8d82-5f1c383cdba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "spec_params = utils.read_spec_params( \"config.ini\" )\n",
    "utils.print_params(spec_params)\n",
    "\n",
    "train_params = utils.read_train_params( \"config.ini\" )\n",
    "utils.print_params(train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3424093-7571-4edf-9d92-042291c421a9",
   "metadata": {},
   "source": [
    "#### Processing step 1.2 - Preparing the Data\n",
    "\n",
    "This is where we actually run the code which parses the filesystem and creates a csv which records all the information required for creating samples (image patches) from the audio data. This step also creates spectrograms with the path to the spectrograms recorded in the csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9484f0-44c1-4e00-ba62-346d358f7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RavenBinaryDataset\n",
    "importlib.reload(RavenBinaryDataset)\n",
    "\n",
    "wav_path_pos = \"./sample_data/whipbird/pos\"\n",
    "wav_path_neg = \"./sample_data/whipbird/neg\"\n",
    "spec_image_dir_path = \"./training_output/preparation/specs\"\n",
    "\n",
    "RavenBinaryDataset.prepare_data( wav_path_pos, wav_path_neg, spec_image_dir_path, spec_params, train_params['dataCSV'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a853a2b-5645-466c-996c-656e2e2d2aac",
   "metadata": {},
   "source": [
    "### Processing step 1.2 - Peruse the results.\n",
    "\n",
    "Look at the CSV file to see what information is contained in it. Also look at the directory on the file system where all of the spectrograms are stored. Open some of the spectrogram images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838f35e",
   "metadata": {},
   "source": [
    "### Balancing dataset \n",
    "\n",
    "The number of examples from each class that the CNN trains on influences the predictions that it makes. If 90% of the examples are positive and 10% are negative, then even a terrible model with no capacity to learn anything about the call itself will eventually optimise its weights to always guess positive and acheive a 90% accuracy. Therefore, we ideally want to provide a similar number of examples from each class for the training examples.  In practice, it's usually a lot easier to source negative examples. Furthermore there is usually more variety amongst the negative examples that we would like to include. \n",
    "\n",
    "We can still include more negative examples than positive examples, and then 'oversample' the positive examples - feeding the CNN positive examples multiple times for each negative example - to ensure that the CNN is fed the same number of positive and negative examples. However, we recommend that this is not done to extreme and you only include at most 2 to 3 times as many negative examples as positive examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f217c-1656-4828-a310-a363725b484c",
   "metadata": {},
   "source": [
    "## 3. Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b2e82-af29-4151-a922-22e0bd1b0852",
   "metadata": {},
   "source": [
    "### Training and Validation split \n",
    "\n",
    "When creating a Machine Learning model, meaning a model that learns from examples, we generally split our data into three sets:\n",
    "1. Training\n",
    "2. Validation\n",
    "3. Test\n",
    "\n",
    "The training set are the examples that are actually used by the algorithm to update the internal parameters.  The simplified overview of training is: \n",
    "1. A prediction (probability of it being positive) is made on a training example.\n",
    "2. The prediction is compared with the true label to get an error\n",
    "3. This error is used to update the weights in a way that would make that prediction better\n",
    "4. Repeat many times with many examples\n",
    "\n",
    "We can look at these predictions as training progresses to see whether it is improving on the training set, but this only tells us how well it is memorising that particular set of examples. What we want to know is how well it works on examples it has never seen before, i.e. if it can recognise the class of audio event (the call type) it is being trained on. \n",
    "\n",
    "Therefore, as training progresses, we use the model to make predictions on the **validation set**. This tells us how well the model does at distinguishing the class of examples it has never \"seen\", which is what we want to know. \n",
    "\n",
    "During the course of developing a recogniser, many modifications to it will be made to what are called \"hyperparameters\", e.g. the spectrogram configuration, the learning rate (more on these later), etc, to try to improve the validation set accuracy. This introduces a chance of \"overfitting\" to the validation set. Just as the CNN algorithm updates its internal parameters based on the training set, we are updating the \"hyperparameters\" based on the validation set. \n",
    "\n",
    "How do we know that the particular values for the hyperparameters is not tuned specifically to our validation set? This is role of the 3rd set, the **test set**. It is a set of data whose only role is to check the accuracy at the end of creating the model, and its accuracy result should not be used to modify any hyperparameters. \n",
    "\n",
    "For this workshop, we won't have the opportunity to spend too long on training and retraining with modified hyperparameters, so we don't worry about the validation set, and the dataset is only divided (randomly) into two parts: *training* and *test*.  We will also use the test set as the validation set - that is to report accuracy during the training process. Since we are not going to be tuning the hyperparameters much during training, this validation accuracy is close to the test accuracy. If we want to be thorough, we should have a third data split which we never look at until the end. \n",
    "\n",
    "What proportion of examples should go into training and into testing? The more we put in training, the better our network will learn. The more we put into testing, the more confident we can be of the reported accuracy on the test set. Generally around 10-20% of the examples in the text set is common. Normally the bigger the total number of examples, the smaller the proportion dedicated to testing. \n",
    "\n",
    "\n",
    "#### Training / Testing cross contamination\n",
    "\n",
    "Ideally, the difference between the training examples and the testing examples should be as great as the difference between the training examples and the audio that will be encountered during deployment (that is when the recogniser is run on unlabelled data collected from the field to gather ecological information). \n",
    "\n",
    "Therefore, if for example you will deploy your recogniser on recordings taken from a location other than where your training recordings were made, then your test set will also come from a different location from your test set. \n",
    "\n",
    "The pipeline used in this workshop randomly divides your recordings into training and testing. That means that, if you have provided files with multiple annotations, all of these annotations will either be used for training or testing. This is what we want, since consecutive vocalisations are likely to be more similar to each other than they are to vocalisations encountered during deployment. If we train on a vocalisation and then test on a consecutive vocalisation from the same individual, this might be an easier task for the recogniser than the kinds of vocalisations it encounters during deployment, and therefore the reported accuracy will be optimistic. \n",
    "\n",
    "However, because the individual recordings are allocated randomly to training or testing, the same problem will occur if these recordings are sourced from the same original longer recordings close in time. This is why we recommended to source your recordings from a wide variety of locations and times of day. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7118d-4199-4120-904e-cce6d349b0a9",
   "metadata": {},
   "source": [
    "### Basic explanation of a CNN\n",
    "\n",
    "(you don't need to know this to build the recogniser, but you might find it interesting)\n",
    "\n",
    "A function is something that maps an input to an output. An example of a function is\n",
    "\n",
    "`f(x1, x2, x3) = a * x1 + b * x2 + c * x3`\n",
    "\n",
    "Here, we have three input variables: `x1`, `x2` and `x3`, and three parameters `a`, `b` and `c`. The output is a single number. \n",
    "\n",
    "By adjusting the parameters `a`, `b` and `c`, we can change how the function behaves. If we had some examples where we knew those three independent variables and the corresponding dependent variable, we could use an algorithm to find the best values for the parameters `a`, `b` and `c`, so that for those examples, the output of the function is as close as possible to the dependent variable.  Because this is a linear function mapping to any real number, this is called linear regression. \n",
    "\n",
    "\n",
    "We could also combine a few of these functions with different paramers so that we map the three input variables to a list of say two output variables. \n",
    "\n",
    "\n",
    "We could then chain these together so that the output values from the first set of linear functions is used as the inputs for the next set, and so on. By putting a non-linear transformation between these layers, we have constructed a big non-linear function. This is a **deep neural network**. At each layer, the number of parameters of the function is the number of input variables multiplied by the number of output variables. \n",
    "\n",
    "A CNN is special type of deep neural network. In our case, the input variables are the pixels of the spectrogram, and the output is  a number which represents the probability of the spectrogram being the positive class. Inside the CNN are many parameters. \n",
    "\n",
    "\n",
    "Unlike the above example, the number of parameters for each output of each layer is **not** the same as the number of input variables. This is because in a CNN, parameters are shared in a special way.  This is necessary because there are so many input parameters (tens of thousands of pixels). \n",
    "\n",
    "\n",
    "We mentioned above that for the very first function we could use an algorithm to find the opitimal values of the parameters to fit our training examples. There is an algorithm called **gradient descent** that can be used to acheive this, and this algorithm is how neural networks learn. \n",
    "\n",
    "1. For each labelled example it sees, it runs it through its big function to produce a number between 0 and 1 representing the probability that the example is a positive example (i.e. it contains your target call). \n",
    "  \n",
    "2. It then compares the calculated probability with the desired probability (i.e. 1 if it was a positive example and 0 if it was a negative example), using something called a \"loss function\" - the output of the loss function for a given example's prediction is called its \"loss\". \n",
    "  \n",
    "3. It then does some calculus to figure out which direction to nudge each parameter so that the loss for that example would be smaller. In a neural network it does this step by step from the last layer to the first layer using a process called \"back propagation\". \n",
    "\n",
    "Actually, the training works in small batches of say 4 or 8 examples. The predictions for all examples in the batch are made, their loss is calculated, and the parameters are updated so that the average loss across the batch is reduced. \n",
    "\n",
    "\n",
    "### Training loss, validation loss, validation accuracy\n",
    "\n",
    "When training, each example will be fed into the CNN many times. Each round of feeding all the examples in is called an **epoch**. After we have fed all the examples in once, we have done one epoch of training. After each epoch we can calculate the average loss across all the examples. Training loss is the average loss across all the training examples. validation loss is the average loss across the validation examples. \n",
    "\n",
    "With each epoch, the training loss should go down, because the training algorithm's job is to reduce this loss. If it doesn't go down then something is quite wrong with the configuration of the network. At some point it will stop improving. \n",
    "\n",
    "What we are more interested in is the validation loss. This will always be higher than the training loss, because these examples were never seen during training. As long as the validation loss is still decreasing, the network is still learning in a way that generalises to examples it's never seen before. At some point the validation loss will stop improving, usually before the training loss does. The validation loss might even increase after a while. This is a sign that the network is overfitting to the training set. This means that it is memorising the individual training examples while reducing its capacity to generalise its understanding of the target call type. \n",
    "\n",
    " The probabilities and loss can be useful for some analysis, but really at the end of the day, the examples have been labelled as binary positive/negative and so what we are interested in is the binary predictions. Generally anything with a predicted probability over 50% is a 'positive' prediction and anything less is a 'negative' prediction.  The accuracy is the proportion of examples that were correctly labelled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a11cc-79f5-407e-9d5e-24c34407035b",
   "metadata": {},
   "source": [
    "### Train the Recogniser\n",
    "\n",
    "There are two main libraries available in python to work with CNNs and other types of artifical neural network: Tensorflow and Pytorch. In this workshop we are using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeeaa23-0956-4bb2-8f4b-c2705557ff15",
   "metadata": {},
   "source": [
    "#### Processing step 2.1 - Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff608d95-9de3-4500-a3b4-82e574dacf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "train_params = utils.read_train_params( \"config.ini\" )\n",
    "utils.print_params(train_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f85805-ebae-4e85-b41f-fa3d25c677c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TrainTest\n",
    "import RavenBinaryDataset\n",
    "import NeuralNets\n",
    "importlib.reload(TrainTest)\n",
    "importlib.reload(RavenBinaryDataset)\n",
    "importlib.reload(NeuralNets)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd4462-926b-412c-87c0-7f6a11dfe7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTest.train(train_params, spec_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30171feb-750d-406f-849c-fa32369e0c88",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "The first step to do after training is to check the examples that the network misclassified. Often we find that some of the training or validation set are labelled incorrectly. It's likely that mislabelled examples are \"misclassified\" by the network (i.e. classified correctly as belonging to a class that does not match the label). We can then correct the label of these examples and retrain. \n",
    "\n",
    "We might also notice that many of the misclassified examples have something in common that indicates an inadequacy with the training/testing dataset. For example we might find that many insect tracks are labelled as positive. This might mean that your positive examples often have insect tracks in the background, and maybe your negative examples don't have insect tracks. You could then go back to your original recordings and extract some segments of the same types of insect tracks to include in your negative examples. \n",
    "\n",
    "You might also find that very faint positive examples are incorrectly labelled as negative. You might be able to improve this false-negative rate by adding more very faint calls into your dataset. However, this is tricky, because this might actually increase the number of false positives. If finding all these very faint calls once the system is deployed is important, then you might just have to add plenty of faint examples and a corresponding number of negative examples that can be confused with faint positive examples. However, it might be that to answer your ecological question, these faint calls are not important. In this case it might be better to simply exclude them completely from the training/testing set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88019ff7-a0f4-4ebd-8c69-f97e514a32b8",
   "metadata": {},
   "source": [
    "## 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f953804-601b-461a-8d9c-d437bea32d49",
   "metadata": {},
   "source": [
    "So far we have trained the network. This took a set of fixed sized spectorgram images and fed them forward to make predictions, then performed backpropagation to update the weights. \n",
    "\n",
    "Inference refers to making predictions on unlabelled data. This is what happens when we deploy our recognizer to do its job. \n",
    "\n",
    "Unlike our train/test set, in deployment we will *normally* be recording longer continuious files, with the intention of locating the target call within these longer files. Therefore, our inference pipeline has a few differences from our training pipeline:\n",
    "\n",
    "- we need to segment the audio recordings\n",
    "- we don't need to calculate loss or do any backpropagation (we can't with no labels)\n",
    "- we need to assemble the individual predictions in a way that gives us information in the context of the original unsegmented audio file. \n",
    "\n",
    "\n",
    "### Segment overlap (temporal precision vs computational work) \n",
    "\n",
    "Our particular type of neural network classifies fixed-length segments of audio. It does not tell us whereabouts within the segment the target call occured. One approach to running this classifier as a call recogniser on a longer file is to split the longer file into non-overlapping consecutive fixed length segment, and then classify each of these. We will then know where the target call has been recognised with a time resolution equal to the duration of the fixed-length segment. \n",
    "\n",
    "Alternatively, we could overlap these segments. For example, we can take a 2.5 second segment every 1 second. This approach makes it less likely that a target call will be missed, since it will appear in several of the segments with varying time-offsets. With non-overlapping segments, many of the target calls will be cut in half across two segments, whereas if we overlap the segments, it means that there is likely to be at least one of the segments that contains a complete enough section of the call to produce a positive classification. \n",
    "\n",
    "The downside of overlapping is that the amount of computation that needs to be performed \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9669f0-218d-4626-b654-22af92dc681e",
   "metadata": {},
   "source": [
    "#### Processing step 3.1 - Doing inference (ie. recognising) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccaa3a6-b360-4aa7-a251-faa5dbdd0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import Inference\n",
    "importlib.reload(utils)\n",
    "importlib.reload(Inference)\n",
    "\n",
    "infer_params = utils.read_infer_params( \"config.ini\" )\n",
    "utils.print_params(infer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8fd35-66a6-4f4f-96a8-263149ee7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inference.do_inference( infer_params, spec_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbd95a-5a5d-4dad-a314-bced8377f635",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e284fe-0745-4982-995b-881cd48acc08",
   "metadata": {},
   "source": [
    "### Verification of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666473f-4bad-46ae-a4ab-205675cb6295",
   "metadata": {},
   "source": [
    "## 5. Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebb41b-d741-4c6a-854d-d3734035605b",
   "metadata": {},
   "source": [
    "### Run Dataset preparation with the enhanced dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0428b-a82a-4f99-bbd7-5652a2ec07fd",
   "metadata": {},
   "source": [
    "### Run training again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fffa6-889a-4448-8ebd-25d803f684ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a5513b-97b6-4e93-9b90-d1b5e3fc47a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c60e64-4119-4276-9dc5-3b17df2972a8",
   "metadata": {},
   "source": [
    "### Things to check\n",
    "\n",
    "- Short audio files \n",
    "- Long Audio files\n",
    "- Audio files with multiple channels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caada921-0317-490c-bd8f-e2531ddf2fda",
   "metadata": {},
   "source": [
    "## Auxilary Code\n",
    "\n",
    "Below are various useful code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f52c7-c549-47c8-bd60-33459f00c251",
   "metadata": {},
   "source": [
    "#### Read config file parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7944c19-716e-4618-a40b-7a1a8cfca135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "# read config with a particular config section, e.g. \"infererence\"\n",
    "inference_params = utils.read_config( \"config.ini\", \"infer\")\n",
    "\n",
    "utils.print_params(inference_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a8520-7ed5-4a00-95ff-7ae90df3f235",
   "metadata": {},
   "source": [
    "#### Load audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e972f-b8b2-4790-ac24-9c7f678460a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_source = \"./sample_data/test/XC201977 - Eastern Whipbird - Psophodes olivaceus.wav\"\n",
    "#file_source = \"\"\n",
    "\n",
    "if not os.path.exists(file_source):\n",
    "    print(\"file doesn't exist\")\n",
    "else:\n",
    "\n",
    "    samples, sample_rate = librosa.core.load(file_source)\n",
    "    #samples, sample_rate = librosa.core.load(file_source, sr=None)\n",
    "    duration = len(samples)/sample_rate\n",
    "\n",
    "    print(str(len(samples)))\n",
    "    print(\"Audio file loaded.\")\n",
    "    print(\"Sample rate : \" + str(sample_rate))\n",
    "    print(\"Duration : \" + str(int(duration)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a96f14-f782-4b51-bf10-7f101bd73950",
   "metadata": {},
   "source": [
    "#### Generate spectrogram from loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588dcbf-c449-4a8d-b8a6-cd31d9f51cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Spectrogram)\n",
    "\n",
    "fftWinSize = 512\n",
    "fftOverlap = 0.5\n",
    "maxFreq = 8000\n",
    "time_win = 2.5\n",
    "spec = Spectrogram.Spectrogram(samples, sample_rate, duration)\n",
    "spec.make_spec( fftWinSize, fftOverlap, maxFreq, time_win)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c4fa8-61e1-4daa-a511-96de35d1896d",
   "metadata": {},
   "source": [
    "#### Get image patch from spectrogram and save image patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a042a88-ef65-4540-a33f-88363dd2e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = spec.get_image_patch( 3.0, 5.0)\n",
    "img.save( \"tmp/test.png\")\n",
    "Image(filename='test.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6551b-e48c-4e0f-8621-0502a5477a69",
   "metadata": {},
   "source": [
    "#### Read Raven File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e06577-8af8-43f5-9cf1-ec6928d3c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "anns = utils.load_Raven_annotations(\"./data/XC201977 - Eastern Whipbird - Psophodes olivaceus.Table.1.selections.txt\")\n",
    "for ann in anns:\n",
    "    print(ann.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813ee25-9ba9-4810-af97-9313f9919315",
   "metadata": {},
   "source": [
    "#### Classify image patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67dfe2-6f73-419a-855f-f775ba932f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NeuralNets\n",
    "\n",
    "net = NeuralNets.CNN_4_Layers( 512, 2, 12, 24, 32, 48)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
