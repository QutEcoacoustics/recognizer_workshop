{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc739ed-43ce-4053-8bec-08754ada91a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# QUT EcoAcoustics Recogniser Workshop 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022292ec-f105-4872-aded-7bf93217dae2",
   "metadata": {},
   "source": [
    "## Overview of the Recogniser Building Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532b255-57a1-4430-a273-b2e43a332c59",
   "metadata": {},
   "source": [
    "### Dataset preparation \n",
    "\n",
    "- Annotating audio\n",
    "- Spectrogram segment generation \n",
    "- Balancing dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902666df-cd1e-4bba-985e-6496421c9389",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- Training/validation split \n",
    "- Running training \n",
    "- error analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb89dba-2b6e-4289-b00b-c63b598cbf93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference\n",
    "\n",
    "- Running inference \n",
    "- inspecting verifying results \n",
    "- labeling data for training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a57074",
   "metadata": {},
   "source": [
    "### Setting up your environment\n",
    "\n",
    "We are going to be using python for everything, including inspecting our dataset, training the recogniser, and visualising spectrograms and results. \n",
    "\n",
    "If are already reading this in your browser, you have python and jupyter installed. In python (like many other languages) there are some core functions available in the language and some that are part of packages that need to be imported before they can be used. Some of those packages are already available in the python distribution, and some packages must be installed before they can be imported. \n",
    "\n",
    "In the cell below, we install all the packages we will need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08933c68-a731-48db-8134-5509906348fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install librosa\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cb321",
   "metadata": {},
   "source": [
    "Before we move on, we need to make sure we are in the correct working directory. By default, the working directory will be the directory this notebook is in which is the 'src' directory. But we want the working directory to be the parent directory, which is the root directory of the repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f00dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.basename(os.path.normpath(os.getcwd())) == 'src':\n",
    "    os.chdir('../')\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f9237",
   "metadata": {},
   "source": [
    "Now that we have installed the required packages, we will import them so that we can use them in our scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f285e-2e25-4d10-a1c6-5d592ca11a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from os import listdir\n",
    "\n",
    "import importlib\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from PIL import Image\n",
    "\n",
    "import configparser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import utils\n",
    "import config\n",
    "import Spectrogram \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8d6f8-8fc5-4bbc-92ff-f1957c161456",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b30bc8",
   "metadata": {},
   "source": [
    "### Dataset labelling\n",
    "\n",
    "The recogniser is is a binary classifier, meaning it assigns one of two labels (positive or negative) to segments of audio.  In a basic Convolutional Neural Network (CNN), these segments are fixed-sized spectrogram images. In the training phase, these fixed-size segments are fed into the CNN along with their labels.  Therefore we need to be have a collection of fixed-length labelled segments. \n",
    "\n",
    "On the page [Dataset Preparation](https://openecoacoustics.org/resources/lessons/make-your-own-recognizer/theory/#1-dataset-preparation) you can read how to prepare the labelled training data in a way that is compatible with the CNN scripts in this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa4b02-7080-41af-91cb-ccab23cf94b0",
   "metadata": {},
   "source": [
    "### Spectrogram Generation\n",
    " \n",
    "The CNN used is an image classifier. The audio signal is converted to an image in the form of a spectrogram.  \n",
    "\n",
    "[Dataset Preparation](https://openecoacoustics.org/resources/lessons/make-your-own-recognizer/theory/#1-dataset-preparation) explains more about the spectrogram. \n",
    "\n",
    "You can edit the parameters used to create the spectrogram i in the configuration file `config.ini`\n",
    "\n",
    "\n",
    "The most important of these is the `time-win` (time window). This sets how many seconds of audio are fed into the CNN. However, note that whatever value is chosen for this, the resulting spectrogram will be reshaped into a size of 128x256 pixels. Setting a very large value for the time window will result in loss of time resolution when this reshaping is done. \n",
    "\n",
    "You can read more on the page [Spectrogram Generation](https://openecoacoustics.org/resources/lessons/make-your-own-recognizer/theory/#1-spectrogram-generation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f8936-55ac-4343-afeb-9ca685e8e80d",
   "metadata": {},
   "source": [
    "### Dataset Preparation - Practical Steps\n",
    "\n",
    "We need to:\n",
    "- 1.1 Initialise configuration parameters.\n",
    "- 1.2 Prepare the training data based on how the training data is layed out on the file system.\n",
    "- 1.3 Check that the information about the training data is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ae1d6-3507-47dd-b1a2-d610df303ebd",
   "metadata": {},
   "source": [
    "#### Processing step 1.1 - Initialise the configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d47a5-fa82-4a18-8d82-5f1c383cdba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "print(os.path.exists(\"data/sample_data/noisypitta/config.ini\"))\n",
    "\n",
    "spec_params = config.read_spec_params( \"data/sample_data/noisypitta/config.ini\" )\n",
    "config.print_params(spec_params)\n",
    "\n",
    "train_params = config.read_train_params( \"data/sample_data/noisypitta/config.ini\" )\n",
    "config.print_params(train_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3424093-7571-4edf-9d92-042291c421a9",
   "metadata": {},
   "source": [
    "#### Processing step 1.2 - Preparing the Data\n",
    "\n",
    "This is where we actually run the code which parses the filesystem and creates a csv which records all the information required for creating samples (image patches) from the audio data. This step also creates spectrograms with the path to the spectrograms recorded in the csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9484f0-44c1-4e00-ba62-346d358f7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RavenBinaryDataset\n",
    "importlib.reload(RavenBinaryDataset)\n",
    "\n",
    "wav_path_pos = \"data/sample_data/noisypitta/pos\"\n",
    "wav_path_neg = \"data/sample_data/noisypitta/neg\"\n",
    "spec_image_dir_path = \"training_output/preparation/noisypitta/specs\"\n",
    "\n",
    "RavenBinaryDataset.prepare_data( wav_path_pos, wav_path_neg, spec_image_dir_path, spec_params, train_params['dataCSV'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a853a2b-5645-466c-996c-656e2e2d2aac",
   "metadata": {},
   "source": [
    "### Processing step 1.2 - Peruse the results.\n",
    "\n",
    "Look at the CSV file to see what information is contained in it. Also look at the directory on the file system where all of the spectrograms are stored. Open some of the spectrogram images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838f35e",
   "metadata": {},
   "source": [
    "[Balancing dataset](https://openecoacoustics.org/resources/lessons/make-your-own-recognizer/theory/#balancing-dataset) explains more about how to balance the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f217c-1656-4828-a310-a363725b484c",
   "metadata": {},
   "source": [
    "## 2. Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b2e82-af29-4151-a922-22e0bd1b0852",
   "metadata": {},
   "source": [
    "### Training and Validation split \n",
    "\n",
    "When creating a Machine Learning model, meaning a model that learns from examples, we generally split our data into three sets:\n",
    "1. Training\n",
    "2. Validation\n",
    "3. Test\n",
    "\n",
    "The training set are the examples that are actually used by the algorithm to update the internal parameters.  The simplified overview of training is: \n",
    "1. A prediction (probability of it being positive) is made on a training example.\n",
    "2. The prediction is compared with the true label to get an error\n",
    "3. This error is used to update the weights in a way that would make that prediction better\n",
    "4. Repeat many times with many examples\n",
    "\n",
    "We can look at these predictions as training progresses to see whether it is improving on the training set, but this only tells us how well it is memorising that particular set of examples. What we want to know is how well it works on examples it has never seen before, i.e. if it can recognise the class of audio event (the call type) it is being trained on. \n",
    "\n",
    "Therefore, as training progresses, we use the model to make predictions on the **validation set**. This tells us how well the model does at distinguishing the class of examples it has never \"seen\", which is what we want to know. \n",
    "\n",
    "How do we know that the particular values for the hyperparameters is not tuned specifically to our validation set? This is role of the 3rd set, the **test set**. It is a set of data whose only role is to check the accuracy at the end of creating the model, and its accuracy result should not be used to modify any hyperparameters. \n",
    "\n",
    "For this workshop, we won't have the opportunity to spend too long on training and retraining with modified hyperparameters, so we don't worry about the validation set, and the dataset is only divided (randomly) into two parts: *training* and *test*.  We will also use the test set as the validation set - that is to report accuracy during the training process. Since we are not going to be tuning the hyperparameters much during training, this validation accuracy is close to the test accuracy. If we want to be thorough, we should have a third data split which we never look at until the end. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7118d-4199-4120-904e-cce6d349b0a9",
   "metadata": {},
   "source": [
    "### Basic explanation of a CNN\n",
    "\n",
    "The CNN is just a very big mathematical function on the inputs to produce an output, which contains a whole lot of parameters. But for understanding how the parameters are learned, it’s not much different from the tiny function in step 1. \n",
    "To get an intuition on how what calculations a CNN is doing to go from a spectrogram to a prediction, we will start with something familiar, a simple linear function and modify it in small steps until we get to the CNN. \n",
    "\n",
    "You don't need to know this to build the recogniser, but you might find it interesting.\n",
    "\n",
    "[Read more about the CNN here](https://openecoacoustics.org/resources/lessons/make-your-own-recognizer/theory/#basic-explanation-of-a-cnn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872aa84b",
   "metadata": {},
   "source": [
    "### Training loss, validation loss, validation accuracy\n",
    "\n",
    "When training, each example will be fed into the CNN many times. Each round of feeding all the examples in is called an **epoch**. After we have fed all the examples in once, we have done one epoch of training. After each epoch we can calculate the average loss across all the examples. Training loss is the average loss across all the training examples. validation loss is the average loss across the validation examples. \n",
    "\n",
    "The validation loss is the most interesting part for us. This will always be higher than the training loss, because these examples were never seen during training. As long as the validation loss is still decreasing, the network is still learning in a way that generalises to examples it's never seen before. At some point the validation loss will stop improving, usually before the training loss does. The validation loss might even increase after a while. This is a sign that the network is overfitting to the training set. This means that it is memorising the individual training examples while reducing its capacity to generalise its understanding of the target call type. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a11cc-79f5-407e-9d5e-24c34407035b",
   "metadata": {},
   "source": [
    "### Train the Recogniser\n",
    "\n",
    "There are two main libraries available in python to work with CNNs and other types of artifical neural network: Tensorflow and Pytorch. In this workshop we are using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeeaa23-0956-4bb2-8f4b-c2705557ff15",
   "metadata": {},
   "source": [
    "#### Processing step 2.1 - Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff608d95-9de3-4500-a3b4-82e574dacf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "train_params = config.read_train_params( \"data/sample_data/noisypitta/config.ini\" )\n",
    "config.print_params(train_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f85805-ebae-4e85-b41f-fa3d25c677c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TrainTest\n",
    "import RavenBinaryDataset\n",
    "import NeuralNets_3FCL as NeuralNets\n",
    "importlib.reload(TrainTest)\n",
    "importlib.reload(RavenBinaryDataset)\n",
    "importlib.reload(NeuralNets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd4462-926b-412c-87c0-7f6a11dfe7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTest.train(train_params, spec_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30171feb-750d-406f-849c-fa32369e0c88",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "The first step to do after training is to check the examples that the network misclassified. Often we find that some of the training or validation set are labelled incorrectly. It's likely that mislabelled examples are \"misclassified\" by the network (i.e. classified correctly as belonging to a class that does not match the label). We can then correct the label of these examples and retrain. \n",
    "\n",
    "We might also notice that many of the misclassified examples have something in common that indicates an inadequacy with the training/testing dataset. For example we might find that many insect tracks are labelled as positive. This might mean that your positive examples often have insect tracks in the background, and maybe your negative examples don't have insect tracks. You could then go back to your original recordings and extract some segments of the same types of insect tracks to include in your negative examples. \n",
    "\n",
    "You might also find that very faint positive examples are incorrectly labelled as negative. You might be able to improve this false-negative rate by adding more very faint calls into your dataset. However, this is tricky, because this might actually increase the number of false positives. If finding all these very faint calls once the system is deployed is important, then you might just have to add plenty of faint examples and a corresponding number of negative examples that can be confused with faint positive examples. However, it might be that to answer your ecological question, these faint calls are not important. In this case it might be better to simply exclude them completely from the training/testing set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ErrorAnalysis\n",
    "importlib.reload(ErrorAnalysis)\n",
    "from IPython import display\n",
    "\n",
    "error_image_path = 'training_output/noisypitta/errors.png'\n",
    "\n",
    "ErrorAnalysis.do_analysis(train_params, spec_params, error_image_path)\n",
    "\n",
    "display.Image(error_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88019ff7-a0f4-4ebd-8c69-f97e514a32b8",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f953804-601b-461a-8d9c-d437bea32d49",
   "metadata": {},
   "source": [
    "So far we have trained the network. This took a set of fixed sized spectorgram images and fed them forward to make predictions, then performed backpropagation to update the weights. \n",
    "\n",
    "Inference refers to making predictions on unlabelled data. This is what happens when we deploy our recogniser to do its job. \n",
    "\n",
    "Unlike our train/test set, in deployment we will *normally* be recording longer continuious files, with the intention of locating the target call within these longer files. Therefore, our inference pipeline has a few differences from our training pipeline:\n",
    "\n",
    "- we need to segment the audio recordings\n",
    "- we don't need to calculate loss or do any backpropagation (we can't with no labels)\n",
    "- we need to assemble the individual predictions in a way that gives us information in the context of the original unsegmented audio file. \n",
    "\n",
    "\n",
    "### Segment overlap (temporal precision vs computational work) \n",
    "\n",
    "Our particular type of neural network classifies fixed-length segments of audio. It does not tell us whereabouts within the segment the target call occured. One approach to running this classifier as a call recogniser on a longer file is to split the longer file into non-overlapping consecutive fixed length segment, and then classify each of these. We will then know where the target call has been recognised with a time resolution equal to the duration of the fixed-length segment. \n",
    "\n",
    "Alternatively, we could overlap these segments. For example, we can take a 2.5 second segment every 1 second. This approach makes it less likely that a target call will be missed, since it will appear in several of the segments with varying time-offsets. With non-overlapping segments, many of the target calls will be cut in half across two segments, whereas if we overlap the segments, it means that there is likely to be at least one of the segments that contains a complete enough section of the call to produce a positive classification. \n",
    "\n",
    "The downside of overlapping is that the amount of computation that needs to be performed. \n",
    "\n",
    "You can adjust the overlap in the `config.ini` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9669f0-218d-4626-b654-22af92dc681e",
   "metadata": {},
   "source": [
    "#### Processing step 3.1 - Doing inference (ie. recognising) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccaa3a6-b360-4aa7-a251-faa5dbdd0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "import Inference\n",
    "importlib.reload(utils)\n",
    "importlib.reload(Inference)\n",
    "\n",
    "infer_params = config.read_infer_params( \"data/sample_data/noisypitta/config.ini\" )\n",
    "config.print_params(infer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646eec3",
   "metadata": {},
   "source": [
    "First find a file you wish to run the recognizer over. There is a file containing noisy pitta here: https://connectqutedu.sharepoint.com/:f:/s/QUTEcoacousticsAnon/EttZnlYpt5FKjtB7JX8IeeABpo4Bhu5xRUradlp-oxaw4w?e=dwcCw8. Save this file to a folder on your computer and then update the function parameter below to point to that folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8fd35-66a6-4f4f-96a8-263149ee7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inference.do_inference( infer_params, spec_params, \"inference_audio/noisypitta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e284fe-0745-4982-995b-881cd48acc08",
   "metadata": {},
   "source": [
    "### Verification of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbd95a-5a5d-4dad-a314-bced8377f635",
   "metadata": {},
   "source": [
    "Our inference script generates a raven annotation file. \n",
    "\n",
    "To check through the results, you can open one of your inference wav files in raven, and then open the annotation file. \n",
    "\n",
    "A couple of things to note:\n",
    "- The temporal \"resolution\" of the annotations will be determined by the segment size and the overlap. \n",
    "- overlapping contiguous positively identified patches will be merged into a single annotation. \n",
    "- there is no frequency information in the annotation, as the recognizer is not designed to detect frequency bounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f770f9b",
   "metadata": {},
   "source": [
    "Look through some of your files to see how the recogniser did. If there are false positives (there probably will be lots), we can take these and put them in our negative set to reduce the false positive rate. \n",
    "\n",
    "In raven, go\n",
    "\n",
    "1. `Go file > selection table`\n",
    "2. Select the tab of the selection table, right click, and rename Tab to 'neg'\n",
    "3. scroll through the file looking for false negative annotations. When you see one, draw a new annotation over the top (while the new 'neg' selection table is active). \n",
    "  - this annotation will be saved as a short audio clip, so make sure the annotation includes some padding around the call. For a 2.5 second CNN input size, around 4 seconds total duration for the annotation works. \n",
    "4. Repeat this until you have a collection of several false positives in your neg selection table. \n",
    "5. Go to `file > save all selections in current table as`\n",
    "  - this will save a folder of short wav files. Rename this file \"neg_iteration_01\"\n",
    "  \n",
    "You might also notice some false negatives (i.e. the target species that was not found by the recogniser). In this case you can do a similar process to above, in a different new selection table. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666473f-4bad-46ae-a4ab-205675cb6295",
   "metadata": {},
   "source": [
    "## 4. Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebb41b-d741-4c6a-854d-d3734035605b",
   "metadata": {},
   "source": [
    "After adding examples to your negative folder (and maybe the positive folder too), scroll up to the top of this workbook and repeat all of the steps from sections 1 (dataset preparation) and 2 (training). \n",
    "\n",
    "See if your accuracy improved. \n",
    "\n",
    "You can then repeat section 3 Inference again, to see if you can improve further. \n",
    "\n",
    "### Proceed with caution\n",
    "\n",
    "This iterative process of adding in examples based on inference output works great, however it can skew things a bit.\n",
    "\n",
    "If your recogniser has poor recall (meaning it misses lots of examples i.e. false negatives), then adding in the false positives to your negative set won't help this.   \n",
    "\n",
    "You can add in the positive detections to your positive set, but the particular examples that it finds are those which it's already good at finding. If there are certain variations that it's good at finding and certain ones that it's bad at finding, it won't improve at those that it has difficulty with. \n",
    "\n",
    "Remember that the training and testing split is done randomly. If you add many new examples in from the same inference file, or the same location, especially if they are near each other within that file, your test/validation set will start to resemble your training set a bit too much to give reliable accuracy.  Remember that ideally we want to test the model on data that is at least as different from your training data as the deployment data. \n",
    "\n",
    "If you want to be thorough about this, you can modify the model so that you manage the training/validation split manually in a principled way, and have a holdout test set at the end of the process from a different set of recordings not used, but that's beyond the scope of this workshop. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
